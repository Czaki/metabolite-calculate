---
- hosts: local
  gather_facts : no
  pre_tasks:
    - name: "loading envirorment"
      tags: always
      set_fact:
        custom_java: '{{ lookup("env", "custom_java") }}'
        download_dir: '{{ lookup("env", "download") }}'
        cluster_dir: '{{ lookup("env", "cluster") }}'
        hadoop_version: '{{ lookup("env", "hadoop_version") }}'
        spark_version: '{{ lookup("env", "spark_version") }}'
        scala_version: '{{ lookup("env", "scala_version") }}'
        sbt_version: '{{ lookup("env", "sbt_version") }}'
        JAVA_HOME: '{{ lookup("env", "JAVA_HOME") }}'
        hadoop_path: '{{ lookup("env", "hadoop_path") }}'
        SPARK_HOME: '{{ lookup("env", "SPARK_HOME") }}'
        SCALA_HOME: '{{ lookup("env", "SCALA_HOME") }}'
        SBT_HOME: '{{ lookup("env", "SBT_HOME") }}'
        master: '{{ lookup("file", "master") }}'
        script_dir: '{{ lookup("env", "script_dir") }}'
        etc_hadoop: '{{ lookup("env", "hadoop_path") }}/etc/hadoop'
        hdfs_dir:  '{{ lookup("env", "hdfs_dir") }}'
        home_dir:  '{{ lookup("env", "home") }}'
        build_dir_calculate:  '{{ lookup("env", "build") }}'
        build_dir_scala:  '{{ lookup("env", "build_scala") }}'
        calculate_src: '{{ lookup("env", "cpp_dir") }}'
        distribute_src: '{{ lookup("env", "source_dir") }}/distribute'
        source_dir: '{{ lookup("env", "source_dir") }}'

  tasks:
    - name: "create download dir"
      file:
        path: '{{ download_dir }}'
        owner: '{{ lookup("env", "USER") }}'
        mode: 0755
        state: directory
    - name: "delete cluster"
      tags:
        - clean
        - setup
      file:
        path: '{{ cluster_dir }}'
        state: absent
    - name: "delete hdfs"
      tags:
        - install
        - setup
      file:
        path: '{{ hdfs_dir }}'
        state: absent
    - name: "create cluster dir"
      tags:
        - clean
        - setup
      file:
        path: '{{ cluster_dir }}'
        owner: '{{ lookup("env", "USER") }}'
        mode: 0755
        state: directory

    - name: "Download Java"
      tags:
        - download
        - setup
      when: 'custom_java == true'
      command: 'wget -nc --no-check-certificate --no-cookies -O {{ download_dir }}/jdk-8u171-linux-x64.tgz --header "Cookie: oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz"'
      ignore_errors: True

    - name: "download hadoop, spark, scala, sbt"
      tags:
        - download
        - setup
      get_url:
            url: '{{ item.src }}'
            dest: '{{ item.dest }}'
      with_items:
        - { dest: '{{ download_dir }}/hadoop-{{ hadoop_version }}.tgz', src: 'http://ftp.man.poznan.pl/apache/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz' }
        - { dest: '{{ download_dir }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz', src: 'http://ftp.man.poznan.pl/apache/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz' }
        - { dest: '{{ download_dir }}/scala-{{ scala_version }}.tgz', src: 'http://downloads.lightbend.com/scala/{{ scala_version }}/scala-{{ scala_version }}.tgz' }
        - { dest: '{{ download_dir }}/sbt-{{ sbt_version }}.tgz', src: 'https://piccolo.link/sbt-{{ sbt_version }}.tgz' }

    - name: "Extract Hadoop, Spark, Scala, sbt"
      tags:
        - unpack
        - setup
      block:
        - name: "Extract Hadoop, Spark, Scala, sbt: Create dirs"
          file:
            path: '{{ item }}'
            state: directory
            mode: 0755
          with_items:
            - '{{ hadoop_path }}'
            - '{{ SPARK_HOME }}'
            - '{{ SCALA_HOME }}'
            - '{{ SBT_HOME }}'

        - name: "Extract Hadoop, Spark, Scala, sbt: unpack"
          unarchive:
            src: '{{ item.tar }}'
            dest: '{{ item.dest }}'
            extra_opts:
              - '--strip-components=1'
          with_items:
            - { tar: '{{ download_dir }}/hadoop-{{ hadoop_version }}.tgz', dest: '{{ hadoop_path }}' }
            - { tar: '{{ download_dir }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz', dest: '{{ SPARK_HOME }}' }
            - { tar: '{{ download_dir }}/scala-{{ scala_version }}.tgz', dest: '{{ SCALA_HOME }}' }
            - { tar: '{{ download_dir }}/sbt-{{ sbt_version }}.tgz', dest: '{{SBT_HOME}}' }

    - name: "Extract Java"
      when: 'custom_java == true'
      tags:
        - unpack
        - setup
      block:
        - name: "Extract Java: create dir"
          file:
            path: '{{ JAVA_HOME }}'
            state: directory
            mode: 0755
        - name: "Extract Java: unpack"
          unarchive:
            src: '{{ download_dir }}/jdk-8u171-linux-x64.tgz'
            dest: '{{ JAVA_HOME }}'
            extra_opts:
              - '--strip-components=1'

    - name: "create cpp build dir"
      tags:
        - build
        - setup
      file:
        path: '{{ build_dir_calculate }}'
        owner: '{{ lookup("env", "USER") }}'
        mode: 0755
        state: directory

    - name: Build calculate
      tags:
        - setup
        - build
      shell: '{{ item }}'
      with_items:
        - 'bash -lc "cd {{ build_dir_calculate }} && cmake -DCMAKE_BUILD_TYPE=Release {{ calculate_src }}"'
        - 'bash -lc "cd {{ build_dir_calculate }} && make -j 4"'
        - 'cp {{ build_dir_calculate }}/metabolite_glpk {{ cluster_dir }}/'

    - name: "create scala build dir"
      tags:
        - build
        - setup
      synchronize:
        src: '{{ distribute_src }}'
        dest: '{{ build_dir_scala }}'

    - name: Build distribute
      tags:
        - setup
        - build
      command: '{{ item }}'
      with_items:
        - 'bash -lc "cd {{ build_dir_scala }}/distribute && {{ SBT_HOME }}/bin/sbt package"'
        - 'cp {{ build_dir_scala }}/distribute/target/scala-2.11/metabolite-distribute_2.11-1.0.jar {{ cluster_dir }}/'
        - 'cp .ivy2/cache/com.github.scopt/scopt_2.11/jars/scopt_2.11-3.7.0.jar {{ cluster_dir }}/'

    - name: Copy data to cluster dir
      tags:
        - setup
        - build
      copy:
        src: '{{ item }}'
        dest: '{{ cluster_dir }}'
      with_items:
        - '{{ source_dir }}/data'
        - '{{ source_dir }}/data-simple'

    - name: Configure hadoop
      command: '{{ item }}'
      tags:
        - install
        - setup
      with_items:
        - 'cp {{ script_dir }}/slaves {{ hadoop_path }}/etc/hadoop/slaves'
        - 'sed -i -e "s|^export JAVA_HOME=\${JAVA_HOME}|export JAVA_HOME={{ JAVA_HOME }}|g" {{ hadoop_path }}/etc/hadoop/hadoop-env.sh'

    - name: Hadop core-site
      tags:
        - install
        - setup
      copy:
       dest: '{{ etc_hadoop }}/core-site.xml'
       content: "<configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master }}:9000</value>
              <description>NameNode URI</description>
            </property>
          </configuration>"

    - name: Hadoop hdfs-site
      tags:
        - install
        - setup
      copy:
        dest: '{{ etc_hadoop }}/hdfs-site.xml'
        content:
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>3</value>
            </property>

            <property>
              <name>dfs.datanode.data.dir</name>
              <value>file://{{ hdfs_dir }}/datanode</value>
              <description>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</description>
            </property>

            <property>
              <name>dfs.namenode.name.dir</name>
              <value>file://{{ hdfs_dir }}/namenode</value>
              <description>Path on the local filesystem where the NameNode stores the namespace and transaction logs persistently.</description>
            </property>

            <property>
              <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
              <value>false</value>
              <description>http://log.rowanto.com/why-datanode-is-denied-communication-with-namenode/</description>
            </property>
          </configuration>

    - name: Hadoop mapred-site
      tags:
        - install
        - setup
      copy:
         dest: '{{ etc_hadoop }}/mapred-site.xml'
         content:
           "<configuration>
               <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
               </property>
           </configuration>"

    - name: Hadoop yarn-site
      tags:
        - install
        - setup
      copy:
        dest: '{{ etc_hadoop }}/yarn-site.xml'
        content:
          "<configuration>
              <property>
                  <name>yarn.nodemanager.aux-services</name>
                  <value>mapreduce_shuffle</value>
              </property>
              <property>
                  <name>yarn.resourcemanager.hostname</name>
                  <value>{{master}}</value>
             </property>
          </configuration>"

    - name: Hadoop hdfs format
      tags:
        - install
        - setup
      command: '{{ hadoop_path }}/bin/hdfs namenode -format -force'

    - name: Spark spark-env
      tags:
        - install
        - setup
      copy:
        dest: '{{ SPARK_HOME }}//conf/spark-env.sh'
        content:
          "#!/usr/bin/env bash \n

          home={{ home_dir }} \n
          export JAVA_HOME={{ JAVA_HOME }} \n
          export SCALA_HOME={{ SCALA_HOME }} \n
          export SBT_HOME={{ SBT_HOME }} \n
          export HADOOP_INSTALL={{ hadoop_path }} \n
          export HADOOP_PREFIX={{ hadoop_path }} \n
          export SPARK_HOME={{ SPARK_HOME }} \n
          export HADOOP_CONF_DIR={{ etc_hadoop }} \n

          export SPARK_WORKER_CORES=`grep -c ^processor /proc/cpuinfo` \n
          MEMORY=`grep  ^MemTotal /proc/meminfo | awk '{rounded = sprintf(\"%.0f\", $2/(1024*1024)/1.5) ; print rounded}'`\n
          #export SPARK_EXECUTOR_MEMORY=\"${MEMORY}g\"\n
          export SPARK_MASTER_HOST={{ master }} \n"
    - name: Spark copy slaves
      tags:
        - install
        - setup
      copy:
        src: '{{ script_dir }}/slaves'
        dest: '{{ SPARK_HOME }}/conf/slaves'

- hosts: remote
  pre_tasks:
    - name: "loading envirorment"
      tags: always
      set_fact:
        cluster_dir: '{{ lookup("env", "cluster") }}'
        hdfs_dir:  '{{ lookup("env", "hdfs_dir") }}'
        home:  '{{ lookup("env", "home") }}'

  tasks:
    - name: "delete cluster"
      tags:
        - coping
        - setup
      file:
        path: '{{ cluster_dir }}'
        state: absent
    - name: "create cluster dir"
      tags:
        - coping
        - setup
      file:
        path: '{{ cluster_dir }}'
        owner: '{{ lookup("env", "USER") }}'
        mode: 0755
        state: directory
    - name: "delete hdfs"
      tags:
        - coping
        - setup
      file:
        path: '{{ hdfs_dir }}'
        state: absent

    - name: Copy build and data
      tags:
        - never
        - build
      copy:
        src: '{{ item }}'
        dest: '{{ cluster_dir }}'
      with_items:
        - '{{ cluster_dir }}/metabolite_glpk'
        - '{{ cluster_dir }}/metabolite-distribute_2.11-1.0.jar'
        - '{{ cluster_dir }}/scopt_2.11-3.7.0.jar'
        - '{{ cluster_dir }}/data'
        - '{{ cluster_dir }}/data-simple'

    - name: Copy additinal files 
      tags:
        - never
        - copy_add
      copy:
        src: '{{ cluster_dir }}/files'
        dest: '{{ cluster_dir }}'

    - name: copy to remotes
      tags:
        - coping
        - setup
      synchronize:
        src: '{{ cluster_dir }}'
        dest: '{{ home }}'

- hosts: local
  pre_tasks:
    - name: "loading envirorment"
      tags: always
      set_fact:
        JAVA_HOME: '{{ lookup("env", "JAVA_HOME") }}'
        hadoop_path: '{{ lookup("env", "hadoop_path") }}'
        SPARK_HOME: '{{ lookup("env", "SPARK_HOME") }}'
        SCALA_HOME: '{{ lookup("env", "SCALA_HOME") }}'
        SBT_HOME: '{{ lookup("env", "SBT_HOME") }}'
  tasks:
    - name: Start cluster
      tags:
          #- setup
        - start_cluster

      shell: '{{ item }}'
      with_items:
        - 'start-dfs.sh'
        - 'start-yarn.sh'
        - 'sleep 30'
        - 'start-all.sh'
      environment:
        PATH: '{{ JAVA_HOME }}/bin:{{ SCALA_HOME}}/bin:{{ SBT_HOME }}/bin:{{ SPARK_HOME }}/bin:{{ SPARK_HOME }}/sbin:{{ hadoop_path }}/bin:{{ hadoop_path }}/sbin:{{ ansible_env.PATH }}'
    - name: stop cluster
      tags:
        - never
        - stop_cluster
      shell: '{{ item }}'
      with_items:
        - 'stop-yarn.sh'
        - 'stop-dfs.sh'
        - 'stop-all.sh'
      environment:
        PATH: '{{ JAVA_HOME }}/bin:{{ SCALA_HOME}}/bin:{{ SBT_HOME }}/bin:{{ SPARK_HOME }}/bin:{{ SPARK_HOME }}/sbin:{{ hadoop_path }}/bin:{{ hadoop_path }}/sbin:{{ ansible_env.PATH }}'
